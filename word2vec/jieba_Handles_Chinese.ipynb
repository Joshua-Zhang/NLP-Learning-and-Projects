{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Chinese does not like English, it cannot split by space, while we deal with the natural language processing, in most cases, we understand article and sentences based on words, so we need a tool to split a article into more specific words.\n",
    "\n",
    "###### So that's the reason why we will use jieba for Chinese, very powerful tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Tokenizer.cut at 0x106c22d58>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /var/folders/vy/jxhn7kh966z583n640xly_8w0000gn/T/jieba.cache\n",
      "Loading model cost 2.234 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 在/ 学习/ 自然/ 自然语言/ 语言/ 处理\n",
      "Default Mode: 我/ 在/ 学习/ 自然语言/ 处理\n",
      "他, 毕业, 于, 上海交通大学, ，, 在, 百度, 深度, 学习, 研究院, 进行, 研究\n",
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 哈佛, 大学, 哈佛大学, 深造\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"我在学习自然语言处理\", cut_all=True)\n",
    "print (seg_list)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"我在学习自然语言处理\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"他毕业于上海交通大学，在百度深度学习研究院进行研究\")  # 默认是精确模式\n",
    "print(\", \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在哈佛大学深造\")  # 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['小明', '硕士', '毕业', '于', '中国科学院', '计算所', '，', '后', '在', '哈佛大学', '深造']\n",
      "小明 硕士 毕业 于 中国科学院 计算所 ， 后 在 哈佛大学 深造\n",
      "小明 硕士 毕业 于 中国 科学 学院 科学院 中国科学院 计算 计算所 ， 后 在 哈佛 大学 哈佛大学 深造\n"
     ]
    }
   ],
   "source": [
    "result_lcut = jieba.lcut(\"小明硕士毕业于中国科学院计算所，后在哈佛大学深造\")\n",
    "print (result_lcut)\n",
    "print (\" \".join(result_lcut))\n",
    "print (\" \".join(jieba.lcut_for_search(\"小明硕士毕业于中国科学院计算所，后在哈佛大学深造\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果/放到/旧/字典/中将/出错/。\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('如果放到旧字典中将出错。', HMM=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.suggest_freq(('中', '将'), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果/放到/旧/字典/中/将/出错/。\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('如果放到旧字典中将出错。', HMM=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "韦少  杜兰特  全明星  全明星赛  MVP  威少  正赛  科尔  投篮  勇士  球员  斯布鲁克  更衣柜  三连庄  NBA  张卫平  西部  指导  雷霆  明星队\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "lines = open('NBA.txt').read()\n",
    "print (\"  \".join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行者  八戒  师父  三藏  唐僧  大圣  沙僧  妖精  菩萨  和尚  那怪  那里  长老  呆子  徒弟  怎么  不知  老孙  国王  一个\n"
     ]
    }
   ],
   "source": [
    "lines = open(u'xiyouji.txt').read()\n",
    "print (\"  \".join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全明星赛  勇士  正赛  指导  对方  投篮  球员  没有  出现  时间  威少  认为  看来  结果  相隔  助攻  现场  三连庄  介绍  嘉宾\n",
      "---------------------我是分割线----------------\n",
      "勇士  正赛  全明星赛  指导  投篮  玩命  时间  对方  现场  结果  球员  嘉宾  时候  全队  主持人  目标  特点  全程  照片  肥皂剧\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "lines = open('NBA.txt').read()\n",
    "print (\"  \".join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))))\n",
    "print (\"---------------------我是分割线----------------\")\n",
    "print (\"  \".join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行者  师父  八戒  三藏  大圣  不知  菩萨  妖精  只见  长老  国王  却说  呆子  徒弟  小妖  出来  不得  不见  不能  师徒\n"
     ]
    }
   ],
   "source": [
    "lines = open(u'xiyouji.txt').read()\n",
    "print (\"  \".join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "爱 v\n",
      "自然语言 l\n",
      "处理 v\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我爱自然语言处理\")\n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "并行分词速度为 238317.1455641935 bytes/second\n",
      "非并行分词速度为 82118.11636596762 bytes/second\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import jieba\n",
    "\n",
    "jieba.enable_parallel()\n",
    "content = open(u'xiyouji.txt',\"r\").read()\n",
    "t1 = time.time()\n",
    "words = \"/ \".join(jieba.cut(content))\n",
    "t2 = time.time()\n",
    "tm_cost = t2-t1\n",
    "print('并行分词速度为 %s bytes/second' % (len(content)/tm_cost))\n",
    "\n",
    "jieba.disable_parallel()\n",
    "content = open(u'xiyouji.txt',\"r\").read()\n",
    "t1 = time.time()\n",
    "words = \"/ \".join(jieba.cut(content))\n",
    "t2 = time.time()\n",
    "tm_cost = t2-t1\n",
    "print('非并行分词速度为 %s bytes/second' % (len(content)/tm_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是默认模式的tokenize\n",
      "自然语言\t\t start: 0 \t\t end:4\n",
      "处理\t\t start: 4 \t\t end:6\n",
      "非常\t\t start: 6 \t\t end:8\n",
      "有用\t\t start: 8 \t\t end:10\n",
      "\n",
      "-----------我是神奇的分割线------------\n",
      "\n",
      "这是搜索模式的tokenize\n",
      "自然\t\t start: 0 \t\t end:2\n",
      "语言\t\t start: 2 \t\t end:4\n",
      "自然语言\t\t start: 0 \t\t end:4\n",
      "处理\t\t start: 4 \t\t end:6\n",
      "非常\t\t start: 6 \t\t end:8\n",
      "有用\t\t start: 8 \t\t end:10\n"
     ]
    }
   ],
   "source": [
    "print (\"这是默认模式的tokenize\")\n",
    "\n",
    "result = jieba.tokenize(u'自然语言处理非常有用')\n",
    "for tk in result:\n",
    "    print(\"%s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))\n",
    "\n",
    "print (\"\\n-----------我是神奇的分割线------------\\n\")\n",
    "\n",
    "print (\"这是搜索模式的tokenize\")\n",
    "result = jieba.tokenize(u'自然语言处理非常有用', mode='search')\n",
    "for tk in result:\n",
    "    print(\"%s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'jieba.analyse' has no attribute 'analyzer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-227dda3a7063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwhoosh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQueryParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0manalyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChineseAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstored\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstored\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstored\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'jieba.analyse' has no attribute 'analyzer'"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "from whoosh.index import create_in,open_dir\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import QueryParser\n",
    "from jieba.analyse.analyzer import ChineseAnalyzer\n",
    "\n",
    "analyzer = jieba.analyse.analyzer.ChineseAnalyzer()\n",
    "schema = Schema(title=TEXT(stored=True), path=ID(stored=True), content=TEXT(stored=True, analyzer=analyzer))\n",
    "    \n",
    "if not os.path.exists(\"tmp\"):\n",
    "    os.mkdir(\"tmp\")\n",
    "\n",
    "ix = create_in(\"tmp\", schema) # for create new index\n",
    "#ix = open_dir(\"tmp\") # for read only\n",
    "writer = ix.writer()\n",
    "\n",
    "writer.add_document(\n",
    "    title=\"document1\",\n",
    "    path=\"/a\",\n",
    "    content=\"This is the first document we’ve added!\"\n",
    ")\n",
    "\n",
    "writer.add_document(\n",
    "    title=\"document2\",\n",
    "    path=\"/b\",\n",
    "    content=\"The second one 你 中文测试中文 is even more interesting! 吃水果\"\n",
    ")\n",
    "\n",
    "writer.add_document(\n",
    "    title=\"document3\",\n",
    "    path=\"/c\",\n",
    "    content=\"买水果然后来世博园。\"\n",
    ")\n",
    "\n",
    "writer.add_document(\n",
    "    title=\"document4\",\n",
    "    path=\"/c\",\n",
    "    content=\"工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作\"\n",
    ")\n",
    "\n",
    "writer.add_document(\n",
    "    title=\"document4\",\n",
    "    path=\"/c\",\n",
    "    content=\"咱俩交换一下吧。\"\n",
    ")\n",
    "\n",
    "writer.commit()\n",
    "searcher = ix.searcher()\n",
    "parser = QueryParser(\"content\", schema=ix.schema)\n",
    "\n",
    "for keyword in (\"水果世博园\",\"你\",\"first\",\"中文\",\"交换机\",\"交换\"):\n",
    "    print(keyword+\"的结果为如下：\")\n",
    "    q = parser.parse(keyword)\n",
    "    results = searcher.search(q)\n",
    "    for hit in results:\n",
    "        print(hit.highlights(\"content\"))\n",
    "    print(\"\\n--------------我是神奇的分割线--------------\\n\")\n",
    "\n",
    "for t in analyzer(\"我的好朋友是李明;我爱北京天安门;IBM和Microsoft; I have a dream. this is intetesting and interested me a lot\"):\n",
    "    print(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
